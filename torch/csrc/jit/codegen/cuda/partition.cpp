#include <torch/csrc/jit/codegen/cuda/partition.h>
#include <ATen/core/jit_type.h>
#include <torch/csrc/jit/codegen/cuda/parser.h>

namespace torch {
namespace jit {
namespace fuser {
namespace cuda {

namespace {

// Check all outputs are:
//   1. TensorType
//   2. on the same device;
// TODO: update this when codegen can output scalar
static c10::optional<c10::Device> getDevice(const Value* value) {
  if (!value->type()->isSubtypeOf(TensorType::get())) {
    // not tensor type, return false as the op is not outputing scalar.
    return c10::nullopt;
  }
  return value->type()->expect<TensorType>()->device();
}

static c10::optional<c10::Device> getDevice(const Node* node) {
  auto outputs = node->outputs();
  for (auto output : outputs) {
    auto device = getDevice(output);
    if (device.has_value()) {
      return device;
    }
  }
  return c10::nullopt;
}

static bool isFusableDevice(const Node* node, const c10::Device device) {
  for (auto value : node->outputs()) {
    auto output_device = getDevice(value);
    if (output_device.has_value() && output_device.value() != device) {
      return false;
    }
  }
  return true;
}

// TODO: we need to check input type when we handle `to()`
static bool isFusableDevice(const Node* node) {
  auto device = getDevice(node);
  if (!device.has_value()) {
    return true;
  }
  return device->is_cuda();
}

inline bool isFusableNode(const Node* node) {
  // checks if node is compatible with parser:
  // 1. if we have a parsing rule; or 2. if the node is already a fusion group.
  return (isNodeParsible(node) || node->kind() == prim::CudaFusionGroup);
}

bool hasReductionOperation(const Node* node) {
  if (isReductionNode(node)) {
    return true;
  }
  if (node->kind() == prim::CudaFusionGroup) {
    for (auto n : node->g(attr::Subgraph)->nodes()) {
      if (hasReductionOperation(n)) {
        return true;
      }
    }
  }
  return false;
}

// TODO: this check is not safe/robust;
// Temporary check to disable different broadcast on branches, this is to avoid
// scheduling issue of inlining TensorView that is broadcasted to different
// shapes. i.e.:
//   inputs(%0, %1, %2)
//   %3 = op1(%0, %1)
//   %4 = op2(%0, %2)
// I don't think we need to worry about broadcasting further down the dependency
// chain, as those would create new IterDomain, which doesn't have th problem of
// conflicting broadcasting?
bool createTrickyBroadcast(const Node* fusion, const Node* node) {
  std::cout << "check multiple broadcast" << std::endl << *fusion << std::endl << *node << std::endl;
  // TORCH_INTERNAL_ASSERT(node->outputs().size() == 1,
  //     "not expecting multiple outputs from a node, graph partitioning logic needs to be updated");

  // // TODO: remove this when scheduler is more robust;
  // // we want to make sure that no intermediate is marked as output
  // // This is to simplify our scheduler.
  // for (const auto& output : node->outputs()) {
  //   for (const auto& use : output->uses()) {
  //     if (use.user != fusion) {
  //       printf("early termination\n");
  //       return true;
  //     }
  //   }
  // }

  auto has_broadcast = [] (const Node* n, const auto& shape) {
      std::cout << "\ncheck node: " << *n << std::endl;
      TORCH_INTERNAL_ASSERT(n->outputs().size() == 1,
          "not expecting multiple outputs from a node, graph partitioning logic needs to be updated");
      // assumes that if output is not a tensor type, it's not broadcasting
      if (auto out_type = n->output(0)->type()->cast<TensorType>()) {
        if (out_type->dim()) {
          if (out_type->dim().value() < shape.size()) {
            // no broadcast for reduction operation;
            printf("lambda no size return false\n");
            return false;
          } else if (out_type->dim().value() > shape.size()) {
            // increased rank means there is reduction;
            printf("lambda rank mismatch return true\n");
            return true;
          } else {
            // same rank, we need to iterate through sizes and check if size-1 exists in input `shape`
            for (int i = 0; i < static_cast<int>(shape.size()); i++) {
              // TODO: not sure if we need to check for output size == 1
              if (shape[i].has_value() && shape[i].value() == 1) {
                printf("lambda size mismatch return true");
                return true;
              }
            }
          }
        }
      }
      printf("lambda return false");
      return false;
  };

  // TODO: this is not necessary if we have smart scheduling!
  // check if merging this code would result in multiple broadcast for a single
  // TensorView within the graph;
  // Because of the producer-consumer relationship, there are only two cases
  // that need to be checked:
  //   case 1. tensor shared as input by `node` & `fusion`;
  //   case 2. tensor generated by `node` and feed as input to `fusion`;

  // case 1. We check shared inputs to `node` & `fusion`;
  for (int i = 0; i < static_cast<int>(node->inputs().size()); i++) {
    auto n_input = node->input(i);
    auto n_input_type = n_input->type()->cast<TensorType>();
    if (n_input_type != nullptr && n_input_type->sizes().sizes()) {
      std::vector<c10::optional<int64_t>> n_input_shape = n_input_type->sizes().sizes().value();
      int num_broadcasting = 0;

      // check broadcasting for n_input inside `node`;
      if (node->kind() == prim::CudaFusionGroup) {
        // be careful here as `subgraph_input`, as its name suggests, is in a
        // different fraph from `node`.
        const auto& subgraph_input = node->g(attr::Subgraph)->inputs()[i];
        for (const auto& use : subgraph_input->uses()) {
          if (has_broadcast(use.user, n_input_shape)) {
            num_broadcasting++;
          }
        }
      } else {
        if (has_broadcast(node, n_input_shape)) {
          num_broadcasting++;
        }
      }

      // check broadcasting for the n_input inside `fusion`;
      for (const auto& use : n_input->uses()) {
        if (use.user == fusion) {
          if (fusion->kind() == prim::CudaFusionGroup) {
            // be careful here as `subgraph_input`, as its name suggests, is in a
            // different fraph from `fusion`.
            const auto& subgraph_input = fusion->g(attr::Subgraph)->inputs()[use.offset];
            for (const auto& use : subgraph_input->uses()) {
              if (has_broadcast(use.user, n_input_shape)) {
                num_broadcasting++;
              }
            }
          } else {
            if (has_broadcast(fusion, n_input_shape)) {
              num_broadcasting++;
            }
          }
        }
      }

      // encounted multiple broadcasting scheme for a single TV, we will not be
			// able to schedule this, prevent the fusion;
      if (num_broadcasting > 1) {
        return true;
      }
    }
  }

  // case 2. We check input to `fusion` that is also the output from `node`
  for (int i = 0; i < static_cast<int>(node->outputs().size()); i++) {
    auto n_output = node->output(i);
    auto n_output_type = n_output->type()->cast<TensorType>();
    if (n_output_type != nullptr && n_output_type->sizes().sizes()) {
      std::vector<c10::optional<int64_t>> n_output_shape = n_output_type->sizes().sizes().value();
      int num_broadcasting = 0;

      // check broadcasting for n_output inside `node`;
      if (node->kind() == prim::CudaFusionGroup) {
        const auto& subgraph_output = node->g(attr::Subgraph)->outputs()[i];
        for (const auto& use : subgraph_output->uses()) {
          // exclude output node
          if (use.user->kind() != prim::Return &&
              has_broadcast(use.user, n_output_shape)) {
            num_broadcasting++;
          }
        }
      }

      // TODO: merge this code with case 1.
      // check broadcasting for the n_output inside `fusion`;
      bool use_as_output = false;
      for (const auto& use : n_output->uses()) {
        if (use.user == fusion) {
          if (fusion->kind() == prim::CudaFusionGroup) {
            // be careful here as `subgraph_input`, as its name suggests, is in a
            // different fraph from `fusion`.
            const auto& subgraph_input = fusion->g(attr::Subgraph)->inputs()[use.offset];
            for (const auto& use : subgraph_input->uses()) {
              if (has_broadcast(use.user, n_output_shape)) {
                num_broadcasting++;
              }
            }
          } else {
            if (has_broadcast(fusion, n_output_shape)) {
              num_broadcasting++;
            }
          }
        } else {
          use_as_output = true;
        }
      }

      // encounted multiple broadcasting scheme for a single TV, we will not be
      // able to schedule this, prevent the fusion;
      if (num_broadcasting > (use_as_output ? 0 : 1)) {
        return true;
      }
    }
  }
  
  return false;
}

} // namespace

bool isFusableCudaFusionGroup(const Node* node) {
  if (isFusableNode(node)) {
    return isFusableDevice(node);
  }
  return false;
}

bool isFusableCudaFusionGroup(const Node* fusion, const Node* node) {
  // TODO: lift the restriction of not fusing producer containing reduction when
  //       we have proper scheduling.
  if (isFusableCudaFusionGroup(node) && !hasReductionOperation(node) &&
      !createTrickyBroadcast(fusion, node) // disabling this due until we properly support it
     ) {
    // TODO: ensure legit fusion.
    // issue 0: currently codegen doesn't support broadcasting, except in the
    //          form of stride 0.
    // We WAR by explicitly extend all tensor to be the broadcasted size. This
    // however requires that we have identical tensor shape for all output
    // tensors.
    // We previously have a check, where for any `node` that we try to fuse, all
    // `auto output : node->outputs(); auto use : output->uses();` has to meet
    // one of the three conditions:
    //   a. use.user == fusion;
    //   b. node->outputs() sizes are compatible with `fusion` outputs;
    //   c. isFusableNode(use.user) && use.user->outputs() sizes are compatible
    //      with `fusion` outputs;
    //
    // However, given the instance of legacy executor, it is not guaranteed the
    // necessary shape information is available to do the check. Hence we are
    // omitting it for now and we'll wait until proper support from profiling is
    // implemented to justify another look at this.
    // And the broadcasting Hack won't be applicable after reduction is
    // supported in codegen. So it's going to be a more complicated story.
    //
    // For now, a voilating fusion would result in no codegen kernel (fallback
    // execution with interpreter and non-optimized graph is used instead)

    // ensure if the node has a designated device, it's on the same device with
    // fusion.
    // TODO: is there a danger of us fusing operations that's supposed to be on
    //       separate GPUs? And is that necessarily bad?
    auto device = getDevice(fusion);
    return (!device.has_value() || isFusableDevice(node, device.value()));
  }
  return false;
}

} // namespace cuda
} // namespace fuser
} // namespace jit
} // namespace torch
